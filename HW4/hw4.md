# HW4: Self-Attention
## 
d_model = 256 (idk)
n_head: 2 -> 4 (conformer)
n_layers: 1 -> 6 (Attention is all you need)
lr = 5e-5 (idk)
adamw betas (0.9, 0.98) (Attention is all you need)